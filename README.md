# NSERC 

Project repository for NSERC summer 2025 \
**Author â­ï¸: Mercury Mcindoe** \
**Team ðŸ€: [Mercury Mcindoe (UBC, BASC)](https://mercurymcindoe.notion.site), [Wan-Cyuan (Chris) Fan (UBC, PHD)](https://sites.google.com/view/wancyuanfan), [Rayat Hossain (UBC, PHD)](https://sites.google.com/view/mirrayatimtiazhossain/home), [Xiangteng He (UBC, PostDoc)](https://hexiangteng.github.io), [Leonid Sigal (UBC, PI)](https://www.cs.ubc.ca/~lsigal/)**

### Conda Environments
---
The Conda environments required for running various scripts (metrics, sampling, etc.) are located in the `conda_envs` directory.

> [!WARNING] 
Environments are not yet merged. Finalized configurations will be added soon.

Install conda environment via : 
```bash
conda env create -f env.yml
```

Most actions can be done with the conda environment `NSERC`. Others are used for metric scripts, look at [`/scripts`](#scripts) for other conda environments.

### Data
---
The `data` directory contains all datasets, generated outputs, and analysis scripts used for evaluation.

**Contents:**
- [Generated Captions](#generated-captions)
- [Images](#images)
- [Scanpaths](#scanpaths)
- [Scripts for Data](#scripts-for-data)
- [Weights](#weights)


#### Generated Captions
---
**Datasets:**
- `CUB_captions`, `MSCOCO2017_val_captions`

**Within each subdirectory:**
- **`generated_captions`** â€” Captions generated per method (**MSCOCO, CUB**)
- **`attributes_eval`** â€” Attribute judgments by LLM (**CUB**)
- **`llm-judge-ratings`** â€” LLM-as-a-judge quality assessments (**MSCOCO, CUB**)
- **`semantics`** â€” Semantic metrics (GEM, CLIP, SBERT) (**MSCOCO, CUB**)
- **`POPE`** â€” Hallucination detection results (**MSCOCO**)


#### Images
---
Stores all image datasets used in experiments.

**Current datasets:**
- `MSCOCO2014`, `MSCOCO2017`, `CUB_200_2011`, `MSCOCO_images`

> [!NOTE] 
`MSCOCO_images` is a curated subset of 1,000 MSCOCO images selected manually.

#### Scanpaths
---
Holds scanpath data generated by the HAT model.

**Structure:**
- `.npy` files containing scanpaths for each image, organized by dataset.
- `images_with_scanpaths` â€” Visualizations overlaying scanpaths on corresponding images.


#### Scripts for Data
---
**Submodules:**
- **`CUB-attribute-utils`** â€” Scripts for handling **CUB_200_2011** attributes:
  - `cub_attribute_convert.py` â€” Computes F1 scores for attribute evaluations in `generated_captions/date/attributes_eval` and outputs a PNG plot.
  - `cub_attribute_gen.py` â€” Creates `attributes.json` for **CUB_200_2011** (currently ignores certainty levels).

- **`weight-utils`** â€” Scripts for attention weight analysis:
  - `head-sim-score.py` â€” Computes salient similarity scores for attention heads; outputs per-image and averaged `.png` and `.csv` files.
  - `head-wise-weight-overlay.py` â€” Overlays attention weights of visual tokens onto images.
  - `weight-analysis.py` â€” Plots heatmaps and score trends from similarity `.csv` files.
  - Includes related `bash` automation scripts.

#### Weights
---
Contains attention weight data for generated captions.  
The `vis` subdirectory stores visualizations of these weights for easier analysis and interpretation.

### Eval
---
The `eval` directory contains all evaluation scripts, organized by downstream task.

> [!NOTE]
Currently, evaluation scripts are only available for **image captioning**.

**Contents:**
- [Hallucination](#hallucination)
- [LLM-as-a-judge](#llm-as-a-judge)
- [Semantics](#semantics)

#### Hallucination
---
Scripts and metrics for detecting object hallucination in image captions.

- **[CHAIR Metric](https://github.com/Maxlinn/CHAIR-metric-standalone)**  
  - `chair-eval.py` â€” Merges CHAIR results into a CSV. 
- **[POPE Hallucination Benchmark](https://github.com/RUCAIBox/POPE)**

#### LLM-as-a-judge
---
Scripts for LLM-based evaluation and judgment of captions.

- `attribute-eval.py` â€” Given a set of captions and attributes, computes False Positives, True Positives, and False Negatives for each caption.
- `gradioClient.py`, `gradioServer.py` â€” Gradio client/server for interactive caption evaluation.
- `judge-prompt.py` â€” Contains prompts used for LLM-based judgments.
- `judge-eval.py` â€” Converts LLM judgments into bar chart visualizations.
- `caption-eval.py` â€” Compares the similarity of generated captions to ground truth caption(s).

#### Semantics
---
Scripts for computing semantic similarity metrics between generated and ground truth captions.

- `clip-eval.py` â€” Computes CLIP score between an image/ground truth caption and a generated caption.
- `sbert-eval.py` â€” Computes SBERT score between a ground truth caption and a generated caption.
- `gem-eval.py` â€” Computes [GEM metrics](https://github.com/GEM-benchmark/GEM-metrics) (BERTScore, BLEU, CIDEr, etc.) between ground truth and generated captions.

### [Human Attention Transformer (HAT)](https://github.com/cvlab-stonybrook/HAT)
---
Model used to generate human gaze scanpaths for each image.

- **`inference.py`** â€” Takes an image as input and outputs an `.npy` file containing the scanpaths for that image, rescaled to the original image resolution.

### [LLaVA](https://llava-vl.github.io)
---
Vision-Language Model (VLM) used for experimentation.

- **[`modifications/`](/NSERC/LLaVA/modifications/README.md)** â€” Contains code for attention manipulation.
  - `modeling_llama.py` â€” Implementation of the LLaMA model. Attention manipulation logic is located in `LLaMAModel`, `LLaMADecoderLayer`, and `LLaMAAttention`.
  - `kwargs_modification.py` â€” Utility for argument bypassing.
### Scripts
---
Contains SLURM and general Bash scripts used for running experiments and evaluations.

**Contents:**
- [Metric Scripts](#metric-scripts)
- [Sampling Scripts](#sampling-scripts)
- [Others](#others)

#### Metric Scripts
---
Bash scripts for running metric evaluations:
- `chair.sh`
- `clip.sh`
- `gem.sh`
- `llm-as-a-judge-attributes.sh`
- `llm-as-a-judge.sh`
- `sbert.sh`

#### Sampling Scripts
---
Bash scripts for generating or sampling data:
- `cococaptions_generate.sh`
- `cubcaptions_generate.sh`
- `HAT.sh`
- `pop_generate.sh`

#### Others
---
Scripts that do not fit into the above categories:
- `cli.sh` â€” Command-line interface tool for interacting with LLaVA.