# Human-Like Attention in Vision Language Models

Project repository for NSERC summer 2025. \
**Author ⭐️: Mercury Mcindoe** \
**Team 🏀: [Mercury Mcindoe (UBC, BASC)](https://mercurymcindoe.notion.site), [Wan-Cyuan (Chris) Fan (UBC, PHD)](https://sites.google.com/view/wancyuanfan), [Rayat Hossain (UBC, PHD)](https://sites.google.com/view/mirrayatimtiazhossain/home), [Xiangteng He (UBC, PostDoc)](https://hexiangteng.github.io), [Leonid Sigal (UBC, PI)](https://www.cs.ubc.ca/~lsigal/)**


### Project Description
---
This project investigates the question:  
**"Can incorporating human-like attention mechanisms or patterns of perception enhance the performance of vision–language models (VLMs)?"**

To explore this, we use approximated gaze points generated by **[HAT](#human-attention-transformer-hat)** and integrate them directly into the attention layers of Transformer architectures.

## Environment Setup
This repository uses multiple submodules and conda environments for different tasks.  
Follow these steps to ensure a complete and working setup.
> [!WARNING] 
Conda environments are not yet merged. Finalized configurations will be added soon.

### 1. Clone the repository with submodules
---
```bash
git clone --recurse-submodules <repo-url>
cd Human-Like-Attention-in-Vision-Language-Models
```
If you already cloned without submodules:
```bash
git submodule update --init --recursive
```

### 2. Set up the main conda environment
---
All conda environments are located within `conda_envs/`. 
The primary environment for most tasks is **NSERC**, defined in `conda_envs/NSERC.yml`.
```bash
conda env create -f conda_envs/NSERC.yml
conda activate NSERC
```
### 3. Metric-specific environments (optional)
---
Some evaluations scripts require separate environments (e.g., GEM, CHAIR, SBERT/CLIP, LLM-as-a-judge).
Create only the ones you need:
```bash
conda env create -f conda_envs/{metric_name}.yml
```
>[!NOTE]
If unsure on what environment is needed, look into `scripts/metric_scripts` to see the environment needed.
### 4. Install local packages (if required)
---
If any submodules contain Python packages, install them in editable mode:
```bash
pip install -e .
pip install -e HAT
```

## Data Preparation
All datasets, generated outputs, and analysis artifacts are stored under the `data` directory. Place your image datasets under `data/images`. Sampling scripts and Evaluation scripts will automatically create the subdirectories.

### 1. Directory Overview
---
The `data` directory contains all datasets, generated outputs, and analysis scripts used for evaluation.

**Contents:**
- [Generated Captions](#generated-captions)
- [Images](#images)
- [Scanpaths](#scanpaths)
- [Scripts for Data](#scripts-for-data)
- [Weights](#weights)


#### Generated Captions
---
**Datasets:**
- `CUB_captions` — Captions for each image within CUB, organized by image id.
- `MSCOCO2017_val_captions` — Captions for images within `MSCOCO_images`, organized by image id.

**Within each subdirectory:**
- **`generated_captions`** — Captions generated per method (**MSCOCO, CUB**)
- **`attributes_eval`** — Attribute judgments by LLM (**CUB**)
- **`llm-judge-ratings`** — LLM-as-a-judge quality assessments (**MSCOCO, CUB**)
- **`semantics`** — Semantic metrics (GEM, CLIP, SBERT) (**MSCOCO, CUB**)
- **`POPE`** — Hallucination detection results (**MSCOCO**)


#### Images
---
Stores all image datasets used in experiments.

**Current datasets in use:**
- `MSCOCO2014`, `MSCOCO2017`, `CUB_200_2011`, `MSCOCO_images`

Place these datasets (or datasets of interest) within the images directory.

> [!NOTE] 
`MSCOCO_images` is a curated subset of 1,000 MSCOCO images selected manually.

#### Scanpaths
---
Holds scanpath data generated by the HAT model.

**Structure:**
- `.npy` files containing scanpaths for each image, organized by dataset.
- `images_with_scanpaths` — Visualizations overlaying scanpaths on corresponding images.

In [`/scripts`](#2-scanpath-generation), you can create scanpaths for your dataset of interest by running the script `HAT.sh`.

#### Scripts for Data
---
**Submodules:**
- **`CUB-attribute-utils`** — Scripts for handling **CUB_200_2011** attributes:
  - `cub_attribute_convert.py` — Computes F1 scores for attribute evaluations in `generated_captions/{date}/attributes_eval` and outputs a PNG plot.
  - `cub_attribute_gen.py` — Creates `attributes.json` for **CUB_200_2011** (currently ignores certainty levels).

- **`weight-utils`** — Scripts for attention weight analysis:
  - `head-sim-score.py` — Computes salient similarity scores for attention heads; outputs per-image and averaged `.png` and `.csv` files.
  - `head-wise-weight-overlay.py` — Overlays attention weights of visual tokens onto images.
  - `weight-analysis.py` — Plots heatmaps and score trends from similarity `.csv` files.
  - Includes related `bash` automation scripts.

#### Weights
---
Contains attention weight data for generated captions.  
The `vis` subdirectory stores visualizations of these weights for easier analysis and interpretation.

## How to Run
This sections lists the main command scripts for generating captions, scanpaths, and evaluation outputs. All scripts are located in the `scripts/` directory unless otherwise specified.

### 1. Caption Generation
---
Generate captions for supported datasets: \
**MSCOCO** 
```bash
bash sampling_scripts/cococaptions_generate.sh
```

**CUB**
```bash
bash sampling_scripts/cubcaptions_generate.sh
```

The outputs would be stored in the destination directory that can be stated within the scripts with the following format :
```json
{
  image_id-1 : [
    caption-1,
    caption-2,
    ...
  ],
  image_id-2 : [
    caption-1,
    caption-2,
    ...
  ],
  ...
}
```

### 2. Scanpath Generation
---
Produce gaze scanpaths for images in a dataset:
```bash
bash sampling_scripts/HAT.sh
```
Outputs `.npy` scanpath files and optional overlays in `data/scanpaths`.

### 3. POPE Hallucination Benchmark
---
Detects object hallucination in image captions.
```bash
bash sampling_scripts/pop_generate.sh
```
> [!NOTE]
This works for MSCOCO dataset. Look into **[POPE](#https://github.com/RUCAIBox/POPE)** for other datasets.

### 4. CHAIR Metric
---
Computes [CHAIR](#https://github.com/Maxlinn/CHAIR-metric-standalone) results for generated image captions and merges it into a csv file.
```bash
bash metric_scripts/chair.sh
```

### 5. GEM Metrics
---
Computes various metrics (BLEU, CIDEr, SPICE etc.) using **[GEM-metrics](#https://github.com/GEM-benchmark/GEM-metrics)**.
```bash
bash metric_scripts/gem.sh
```

### 6. Sentence Transformers
---
Computes the **[SBert](#https://sbert.net)** F1 score. 
```bash
bash metric_scripts/sbert.sh
```
### 7. LLM-as-a-judge
---
In this project, LLMs were used as a judge to:
1) Evaluate caption quality
2) Measure attribute count (**CUB**). 

The currently used model is **QWen3-32B**, you may change the model if wanted in the `eval/captions/llm-as-a-judge` directory.

```bash
bash metric_scripts/llm-as-a-judge.sh
bash metric_scripts/llm-as-a-judge-attributes.sh
```

### 8. Interactive CLI with LLaVA
---
You can run the model interactively via command line interface.
```bash
bash others/cli.sh
```
For more information about this mode, refer to the **[LLaVA](https://github.com/haotian-liu/LLaVA)** repository for details.

## Evaluation

The `eval` directory contains all evaluation scripts, organized by downstream task.

> [!NOTE]
Currently, evaluation scripts are only available for **image captioning**.

**Contents:**
- [Hallucination](#hallucination)
- [LLM-as-a-judge](#llm-as-a-judge)
- [Semantics](#semantics)

#### Hallucination
---
Scripts and metrics for detecting object hallucination in image captions.

- **[CHAIR Metric](https://github.com/Maxlinn/CHAIR-metric-standalone)**  
  - `chair-eval.py` — Merges CHAIR results into a CSV. 
- **[POPE Hallucination Benchmark](https://github.com/RUCAIBox/POPE)**

#### LLM-as-a-judge
---
Scripts for LLM-based evaluation and judgment of captions.

- `attribute-eval.py` — Given a set of captions and attributes, computes False Positives, True Positives, and False Negatives for each caption.
- `gradioClient.py`, `gradioServer.py` — Gradio client/server for interactive caption evaluation.
- `judge-prompt.py` — Contains prompts used for LLM-based judgments.
- `judge-eval.py` — Converts LLM judgments into bar chart visualizations.
- `caption-eval.py` — Compares the similarity of generated captions to ground truth caption(s).

#### Semantics
---
Scripts for computing semantic similarity metrics between generated and ground truth captions.

- `clip-eval.py` — Computes CLIP score between an image/ground truth caption and a generated caption.
- `sbert-eval.py` — Computes SBERT score between a ground truth caption and a generated caption.
- `gem-eval.py` — Computes [GEM metrics](https://github.com/GEM-benchmark/GEM-metrics) (BERTScore, BLEU, CIDEr, etc.) between ground truth and generated captions.

## Models
###  1. [Human Attention Transformer (HAT)](https://github.com/cvlab-stonybrook/HAT)
---
Model used to generate human gaze scanpaths for each image.

- **`inference.py`** — Takes an image as input and outputs an `.npy` file containing the scanpaths for that image, rescaled to the original image resolution.

### 2. [LLaVA](https://llava-vl.github.io)
---
Vision-Language Model (VLM) used for experimentation.

- **[`modifications/`](/Human-Like-Attention-in-Vision-Language-Models/LLaVA/modifications/README.md)** — Contains code for attention manipulation.
  - `modeling_llama.py` — Implementation of the LLaMA model. Attention manipulation logic is located in `LLaMAModel`, `LLaMADecoderLayer`, and `LLaMAAttention`.
  - `kwargs_modification.py` — Utility for argument bypassing.
